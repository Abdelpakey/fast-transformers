Fast Transformers
=================

This python package provides several implementations of fast self attention in
order to build transformer networks that scale to large sequences.

Links to the relevant ongoing research as well as code examples will be added
in the future.
